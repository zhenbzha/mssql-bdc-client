{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark"
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 2
            },
            "pygments_lexer": "python2"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<img src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n",
                "<br>\r\n",
                "\r\n",
                "# SQL Server 2019 big data cluster Tutorial\r\n",
                "## 04 - Using Spark for ETL\r\n",
                "\r\n",
                "In this tutorial you will learn how to work with Spark Jobs in a SQL Server big data cluster. \r\n",
                "\r\n",
                "Many times Spark is used to do transformations on data at large scale. In this Jupyter Notebook, you'll read a large text file into a Spark DataFrame, and then save out the top 10 examples as a table using SparkSQL.\r\n",
                "\r\n",
                "Make sure to set your kernel to PySpark.  This should attach you to the correct connection, which is the cluster ip address at port 31433, and the default database."
            ],
            "metadata": {
                "azdata_cell_guid": "76df4eab-04c9-4c02-935e-cefa994031b4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Read the product reviews CSV files into a spark data frame, print schema & top rows\r\n",
                "results = spark.read.option(\"inferSchema\", \"true\").csv('/product_review_data').toDF(\"Item_ID\", \"Review\")"
            ],
            "metadata": {
                "azdata_cell_guid": "26390261-10e8-4d7f-bfff-922dacb35d61",
                "tags": []
            },
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "# Save results as parquet file and create hive table\r\n",
                "results.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"Top_Product_Reviews\")"
            ],
            "metadata": {
                "azdata_cell_guid": "0f8978c9-5dd8-4bcc-af1a-26f1aa03b0c2"
            },
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                "# Execute Spark SQL commands\r\n",
                "sqlDF = spark.sql(\"SELECT * FROM Top_Product_Reviews LIMIT 10\")\r\n",
                "sqlDF.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "f0de90b3-a8ac-423f-8bfc-c45b36a70e84"
            },
            "outputs": [],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Next Steps: Continue the Hands-on Lab\r\n",
                "\r\n",
                "Continue the Hands-on Lab now that you have the basics of running notebooks on big data clusters to perform a variety of computations and workloads. The next exercise will walk you through performing machine learning on the cluster."
            ],
            "metadata": {
                "azdata_cell_guid": "f96696ed-bbf4-432b-8cec-93d1b7da7484"
            }
        }
    ]
}